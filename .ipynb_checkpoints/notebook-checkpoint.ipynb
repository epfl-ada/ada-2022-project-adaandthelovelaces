{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook of Milestone 2 of team _Ada And the Lovelaces_. The outline of the notebook is as followed:\n",
    "1. Data Pre-Processing: we add the features of interest to the existing datasets\n",
    "2. Exploratory Data Analysis: based on the datasets obtained, we run some first basic analysis and visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.parse as urlparse\n",
    "from collections import defaultdict\n",
    "\n",
    "from bokeh.models import Range1d, Circle, ColumnDataSource, MultiLine\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import from_networkx\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "import os\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing clicks, impressions and CTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done at two different granularities:\n",
    "- Per source: we compute the clicks, impressions and CTR for each outgoing links of a given article\n",
    "- Aggregated: we compute the total number of clicks, impressions and CTR for the articles no matter the origin of the user's click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine grained dataset: per source article\n",
    "Here, the goal is to extend the _links_ table by adding for each pair _(origin article, destination article)_, columns counting the number of clicks associated, the number of impressions, and the CTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def title_parse(title):\n",
    "    \"\"\"\n",
    "    Parse URL encoded to readable characters\n",
    "    \"\"\"\n",
    "\n",
    "    title = urlparse.unquote(title).casefold()\n",
    "    return title\n",
    "\n",
    "\n",
    "\n",
    "def url_encode(title):\n",
    "    \"\"\"\n",
    "    URL encode target links\n",
    "    \"\"\"\n",
    "\n",
    "    title = urlparse.quote(title)\n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by retrieving the players's data: they are in the paths_finished and paths_unfinished datasets. We first focus on the finished paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_finished = pd.read_csv('./data/wikispeedia_paths-and-graph/paths_finished.tsv',\n",
    "                      sep='\\t',\n",
    "                      names=['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating'],\n",
    "                      comment='#')\n",
    "\n",
    "# It was noticed that users clicked on links in the footer (Terms & Conditions + Disclaimer) \n",
    "# that are not articles per-say. We decided to remove all paths containing these links as it is not in the spirit of the game\n",
    "## \"wikipedia_text_of_the_gnu_free_documentation_license\" is not an article!\n",
    "problematic_keywords = ['Wikipedia_Text_of_the_GNU_Free_Documentation_License']\n",
    "df_user_finished = df_user_finished[~df_user_finished.path.str.contains('|'.join(problematic_keywords))]\n",
    "\n",
    "# Drop unused columns for objectives and transform to list of paths\n",
    "df_user_finished = df_user_finished.drop(['hashedIpAddress', 'timestamp', 'durationInSec', 'rating'], axis=1).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now enrich our series_paths dataframe with the unfinished paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_unfinished = pd.read_csv('./data/wikispeedia_paths-and-graph/paths_unfinished.tsv',\n",
    "                      sep='\\t',\n",
    "                      names=[\"hashedIpAddress\", \"timestamp\", \"durationInSec\", \"path\", \"target\", \"type\"],\n",
    "                      comment='#')\n",
    "\n",
    "# It was noticed that users clicked on links in the footer (Terms & Conditions + Disclaimer) \n",
    "# that are not articles per-say. We decided to remove all paths containing these links as it is not in the spirit of the game\n",
    "## \"wikipedia_text_of_the_gnu_free_documentation_license\" is not an article!\n",
    "problematic_keywords = ['Wikipedia_Text_of_the_GNU_Free_Documentation_License']\n",
    "\n",
    "df_user_unfinished = df_user_unfinished[~df_user_unfinished.path.str.contains('|'.join(problematic_keywords))]\n",
    "\n",
    "df_user_unfinished = df_user_unfinished.drop(['hashedIpAddress', 'timestamp', 'durationInSec', 'target', 'type'], axis=1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo Parent\\AppData\\Local\\Temp\\ipykernel_8076\\32903713.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_user = df_user_finished.append(df_user_unfinished)#Need to chance to pandas concat TODO\n"
     ]
    }
   ],
   "source": [
    "df_user = df_user_finished.append(df_user_unfinished)#Need to chance to pandas concat TODO\n",
    "# TODO Remove problematic artilces here ONCE\n",
    "# Decode URL encoding for paths, case fold\n",
    "series_paths = df_user.map(title_parse)\n",
    "\n",
    "# Transform paths to list of inputs (article names or <)\n",
    "series_paths = series_paths.str.split(';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the new columns of interest in the df_links dataframe, and initialize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract list of links as DataFrame to store frequency\n",
    "\n",
    "df_links = pd.read_csv('./data/wikispeedia_paths-and-graph/links.tsv',\n",
    "                       sep='\\t',\n",
    "                       names=['linkSource_URL', 'linkTarget_URL'],\n",
    "                       comment='#')\n",
    "# Definitions\n",
    "# Source Links are the link of the page on which a link is shown\n",
    "# Target Links are the outgoing link on which the user lands\n",
    "\n",
    "# Filenames are single encoded (ie C++.htm is encoded at C%2B%2B.htm)\n",
    "# Outgoing names in the HTML source are DOUBLE encoded (ie C++.htm is <a href=\"../../wp/c/C%252B%252B.htm\">\n",
    "# Therefore, we add a helper column for linkTarget_2URL storing the DOUBLE URL ENCODED string\n",
    "df_links['linkTarget_2URL'] = df_links['linkTarget_URL'].map(url_encode)\n",
    "df_links['linkSource_2URL'] = df_links['linkSource_URL'].map(url_encode)\n",
    "\n",
    "# Decode URL encoding for paths, case fold (from URL encoding to readable)\n",
    "df_links['linkSource'] = df_links['linkSource_URL'].map(title_parse)\n",
    "df_links['linkTarget'] = df_links['linkTarget_URL'].map(title_parse)\n",
    "\n",
    "# Merge columns in tuple for easy lookup (unique key), format source;target\n",
    "df_links['linkPair'] = df_links['linkSource'] + ';' + df_links['linkTarget']\n",
    "\n",
    "# Number of impressions will be counted in column 'impressions' for each pair\n",
    "df_links['impressions'] = pd.Series(np.zeros((len(df_links)),dtype=int))\n",
    "\n",
    "# Number of hits will be counted in column 'hits' for each pair\n",
    "df_links['clicks'] = pd.Series(np.zeros((len(df_links)),dtype=int))\n",
    "\n",
    "# Storing the x and y position of the unique link on the page\n",
    "df_links['xpos'] =  pd.Series(np.zeros((len(df_links)),dtype=int))\n",
    "df_links['ypos'] =  pd.Series(np.zeros((len(df_links)),dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Extract list of articles\n",
    "df_articles = pd.read_csv('./data/wikispeedia_paths-and-graph/articles.tsv',\n",
    "                          sep='\\t',\n",
    "                          names=['title'],\n",
    "                          comment='#')\n",
    "series_articles_URL = df_articles['title']\n",
    "series_articles_2URL = series_articles_URL.map(url_encode)\n",
    "# Decode URL encoding for paths, case fold\n",
    "series_articles = df_articles['title'].map(title_parse)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code serves for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization step, preprocessing indices to use numpy arrays for faster access\n",
    "# For all articles names, create a list of rows in the df_links that have this article as source\n",
    "source_map = defaultdict(list)\n",
    "for link_id, source in enumerate(df_links['linkSource']):\n",
    "    source_map[source].append(link_id)\n",
    "\n",
    "# Create a reverse dictionary to index nparray\n",
    "# list[int]=string to dict[string]=int\n",
    "my_map = dict(df_links['linkPair'])\n",
    "pair_map = dict((v, k) for k, v in my_map.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of the number of clicks and impressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid pair: 'finland;åland'\n",
      "Invalid pair: 'finland;åland'\n",
      "Invalid pair: 'republic_of_ireland;éire'\n",
      "Invalid pair: 'republic_of_ireland;éire'\n",
      "Invalid pair: 'claude_monet;édouard_manet'\n",
      "Invalid pair: 'republic_of_ireland;éire'\n",
      "Invalid pair: 'claude_monet;édouard_manet'\n",
      "Invalid pair: 'republic_of_ireland;éire'\n",
      "Invalid pair: 'finland;åland'\n",
      "Invalid pair: 'finland;åland'\n",
      "Invalid pair: 'ireland;éire'\n",
      "Invalid pair: 'republic_of_ireland;éire'\n",
      "Invalid pair: 'ireland;éire'\n",
      "Invalid pair: 'impressionism;édouard_manet'\n",
      "Invalid pair: 'claude_monet;édouard_manet'\n",
      "Invalid pair: 'francisco_goya;édouard_manet'\n",
      "Invalid pair: 'republic_of_ireland;éire'\n",
      "Invalid pair: 'ireland;éire'\n",
      "Invalid pair: 'ireland;éire'\n",
      "Invalid pair: 'republic_of_ireland;éire'\n",
      "Invalid pair: 'claude_monet;édouard_manet'\n",
      "Invalid pair: 'republic_of_ireland;éire'\n"
     ]
    }
   ],
   "source": [
    "np_links = df_links.to_numpy()\n",
    "IMPRESSIONS_COLUMN_ID = df_links.columns.get_loc('impressions')\n",
    "CLICKS_COLUMN_ID = df_links.columns.get_loc('clicks')\n",
    "XPOS_COLUMN_ID = df_links.columns.get_loc('xpos')\n",
    "YPOS_COLUMN_ID = df_links.columns.get_loc('ypos')\n",
    "\n",
    "# For each path, count forward link clicks\n",
    "for path in series_paths:\n",
    "    # Initialise stack with first link\n",
    "    foo = list()\n",
    "    foo.append(path[0])\n",
    "    # Iterate over every step of user's path\n",
    "    for element in path[1:]:\n",
    "        if element != '<':\n",
    "            # If next element in path is not a return character, store information and analyze\n",
    "            ## Source node is the current top of stack\n",
    "            source_node = foo[-1]\n",
    "\n",
    "            ## Add next link in list to top of stack\n",
    "            foo.append(element)\n",
    "\n",
    "            ## New top of stack is target\n",
    "            target_node = foo[-1]\n",
    "\n",
    "            # Create key for pair identification\n",
    "            search_value = source_node + ';' + target_node\n",
    "\n",
    "            # Some issues were discovered with the dataset and encoding: some link pairs are missing\n",
    "            ### ie 'finland;åland' is not in our dataset, BUT the link 'åland' out of finland EXISTS on the source article and was used by the user\n",
    "            # It was not listed as an outgoing link in the dataset\n",
    "            try:\n",
    "                pair = pair_map[search_value]\n",
    "            except Exception as e:\n",
    "                # Catch when pair was not listed in dataset, don't add impression and disregard\n",
    "                print('Invalid pair:',e) # For debug and understanding dataset\n",
    "                break\n",
    "\n",
    "            ## Count one impression for all pairs with source_node as source (source_node;*)\n",
    "            source = source_map[source_node]\n",
    "            np_links[source,IMPRESSIONS_COLUMN_ID] += 1\n",
    "\n",
    "            ## Add one click-through for the pair (source_node;target_node)\n",
    "            np_links[pair,CLICKS_COLUMN_ID] += 1\n",
    "\n",
    "        else:\n",
    "            # If return character is read, pop top of stack and don't store any info\n",
    "            foo.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put back the values obtained in np_links in the df_links dataframe, since dataframes are easier to handle than series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_links.columns\n",
    "to_dict_values = {}\n",
    "\n",
    "\n",
    "for i in range(len(columns)):\n",
    "\n",
    "    col = columns[i]\n",
    "    to_dict_values[col] = [np_links[k][i] for k in range(len(np_links))]\n",
    "\n",
    "df_links = pd.DataFrame(to_dict_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can finally compute the CTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links['CTR'] = df_links.clicks / df_links.impressions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing the location of the links in the HTML pages\n",
    "\n",
    "In this part, we compute the location of every link in a given HTML page. The location is added to the _links_ table."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start by extracting the links, and creating the new columns we wish to add, with 0 as the initialization value:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the HTML driver is created. We need to define arbitrary parameters. We consider a full screen display at 1900*900, rendered in Google Chrome.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_Note that if we were to change the resolution, the data would change but the ordering of the links locations on the y axis would remain unchanged. The results of the analysis would not change._"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "op = Options()\n",
    "op.add_argument('headless')\n",
    "op.add_argument(\"window-size=1920,900\") # Most common window size for desktop, full-screen minus 130 at top for browser control, minus 50 at bottom for windows taskbar. Since the game was played, on desktop, we choose this (ADD EXPLANATORY SCREENSHOT)\n",
    "driver = webdriver.Chrome(options=op)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the driver is configured, let's compute the locations of all links in the articles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Article: %25C3%2581ed%25C3%25A1n_mac_Gabr%25C3%25A1in 0 / 4604\n",
      "Current Article: %25C3%2585land 1 / 4604\n",
      "Current Article: %25C3%2589douard_Manet 2 / 4604\n",
      "Current Article: %25C3%2589ire 3 / 4604\n",
      "Current Article: %25C3%2593engus_I_of_the_Picts 4 / 4604\n",
      "Current Article: %25E2%2582%25AC2_commemorative_coins 5 / 4604\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [37]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     14\u001B[0m source_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfile://\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39muser_cwd\u001B[38;5;241m+\u001B[39msource_path\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m#Open the browser\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[43mdriver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m#List of target links we want to find positions for\u001B[39;00m\n\u001B[0;32m     19\u001B[0m list_target_articles \u001B[38;5;241m=\u001B[39m df_links\u001B[38;5;241m.\u001B[39mloc[df_links[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinkSource_2URL\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m==\u001B[39msource][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinkTarget_2URL\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:455\u001B[0m, in \u001B[0;36mWebDriver.get\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    452\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;124;03m    Loads a web page in the current browser session.\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 455\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCommand\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGET\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43murl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:442\u001B[0m, in \u001B[0;36mWebDriver.execute\u001B[1;34m(self, driver_command, params)\u001B[0m\n\u001B[0;32m    439\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msessionId\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m params:\n\u001B[0;32m    440\u001B[0m         params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msessionId\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession_id\n\u001B[1;32m--> 442\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommand_executor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdriver_command\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response:\n\u001B[0;32m    444\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_handler\u001B[38;5;241m.\u001B[39mcheck_response(response)\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:294\u001B[0m, in \u001B[0;36mRemoteConnection.execute\u001B[1;34m(self, command, params)\u001B[0m\n\u001B[0;32m    292\u001B[0m data \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mdump_json(params)\n\u001B[0;32m    293\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 294\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand_info\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:316\u001B[0m, in \u001B[0;36mRemoteConnection._request\u001B[1;34m(self, method, url, body)\u001B[0m\n\u001B[0;32m    313\u001B[0m     body \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeep_alive:\n\u001B[1;32m--> 316\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    317\u001B[0m     statuscode \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mstatus\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\urllib3\\request.py:78\u001B[0m, in \u001B[0;36mRequestMethods.request\u001B[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_encode_url(\n\u001B[0;32m     75\u001B[0m         method, url, fields\u001B[38;5;241m=\u001B[39mfields, headers\u001B[38;5;241m=\u001B[39mheaders, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39murlopen_kw\n\u001B[0;32m     76\u001B[0m     )\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_encode_body(\n\u001B[0;32m     79\u001B[0m         method, url, fields\u001B[38;5;241m=\u001B[39mfields, headers\u001B[38;5;241m=\u001B[39mheaders, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39murlopen_kw\n\u001B[0;32m     80\u001B[0m     )\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\urllib3\\request.py:170\u001B[0m, in \u001B[0;36mRequestMethods.request_encode_body\u001B[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001B[0m\n\u001B[0;32m    167\u001B[0m extra_kw[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mupdate(headers)\n\u001B[0;32m    168\u001B[0m extra_kw\u001B[38;5;241m.\u001B[39mupdate(urlopen_kw)\n\u001B[1;32m--> 170\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murlopen(method, url, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mextra_kw)\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\urllib3\\poolmanager.py:376\u001B[0m, in \u001B[0;36mPoolManager.urlopen\u001B[1;34m(self, method, url, redirect, **kw)\u001B[0m\n\u001B[0;32m    374\u001B[0m     response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(method, url, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    375\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 376\u001B[0m     response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(method, u\u001B[38;5;241m.\u001B[39mrequest_uri, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    378\u001B[0m redirect_location \u001B[38;5;241m=\u001B[39m redirect \u001B[38;5;129;01mand\u001B[39;00m response\u001B[38;5;241m.\u001B[39mget_redirect_location()\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m redirect_location:\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_proxy(conn)\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    704\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    705\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    706\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    711\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[0;32m    714\u001B[0m \u001B[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[0;32m    715\u001B[0m \u001B[38;5;66;03m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[0;32m    716\u001B[0m \u001B[38;5;66;03m# mess.\u001B[39;00m\n\u001B[0;32m    717\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    444\u001B[0m             httplib_response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    445\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    446\u001B[0m             \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    447\u001B[0m             \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    448\u001B[0m             \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m--> 449\u001B[0m             \u001B[43msix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[1;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[1;34m(value, from_value)\u001B[0m\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    441\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    442\u001B[0m     \u001B[38;5;66;03m# Python 3\u001B[39;00m\n\u001B[0;32m    443\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 444\u001B[0m         httplib_response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    445\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    447\u001B[0m         \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    448\u001B[0m         \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[0;32m    449\u001B[0m         six\u001B[38;5;241m.\u001B[39mraise_from(e, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\http\\client.py:1377\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1375\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1376\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1377\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1378\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[0;32m   1379\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\http\\client.py:320\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 320\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    321\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[0;32m    322\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\http\\client.py:281\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 281\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    282\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[0;32m    283\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\socket.py:704\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    703\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 704\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    706\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "article_counter = 0\n",
    "total = len(series_articles_2URL)\n",
    "\n",
    "for source in series_articles_2URL:\n",
    "    #source=series_articles_2URL[0] #DEBUG\n",
    "    print('Current Article:',source,article_counter,'/',total )\n",
    "    article_counter+=1 #For progress tracking\n",
    "\n",
    "    #Compute relative path to html\n",
    "    source_path = '/data/wpcd/wp/' + source[0].lower() + '/' + source + '.htm'\n",
    "\n",
    "    #Absolute path to html file\n",
    "    user_cwd = os.getcwd().replace('\\\\','/')\n",
    "    source_url = 'file://'+user_cwd+source_path\n",
    "    #Open the browser\n",
    "    driver.get(source_url)\n",
    "\n",
    "    #List of target links we want to find positions for\n",
    "    list_target_articles = df_links.loc[df_links['linkSource_2URL']==source]['linkTarget_2URL']\n",
    "\n",
    "    for target in list_target_articles:\n",
    "        #The pairs are in readable characters, target is double encoded format\n",
    "        target_readable = title_parse(title_parse(target))\n",
    "        source_readable = title_parse(title_parse(source))\n",
    "        search_pair = source_readable + ';' + target_readable\n",
    "\n",
    "        try:\n",
    "            current_pair = pair_map[search_pair]\n",
    "        except Exception as e:\n",
    "            # Catch when pair was not listed in dataset\n",
    "            print('Invalid pair:',e) # For debug and understanding dataset\n",
    "            break\n",
    "\n",
    "        #Find the href with /target.htm in all elements of page\n",
    "        #Example with target = World War I, we are looking for /World_War_I.htm\n",
    "        href=driver.find_elements(By.XPATH,  \".//a[contains(@href,'/\" + target + \".htm')]\")\n",
    "        try:\n",
    "            df_links.loc[current_pair, 'xpos']=href[0].location['x']\n",
    "            df_links.loc[current_pair, 'ypos']=href[0].location['y']\n",
    "            #DEBUG\n",
    "            #if len(href):\n",
    "            #Rajouter dans une colonne occurence\n",
    "            #print('Target',target,'appeared',len(href),'times')\n",
    "        except Exception as e:\n",
    "            print('Not found:',target)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving the obtained dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "df_links.to_csv('./output/output_links.csv',encoding='utf-8-sig')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Coarse-grained dataset: Aggregated view over all website\n",
    "Here, the goal is to extend the _articles_ table by adding columns representing the total number of clicks, the number of impressions and the CTR. Except for the CTR, we can simply sum the obtained numbers in the finer granularity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's extract the articles data by aggregating the df_links dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "articles = df_links[[\"linkTarget\", \"impressions\", \"clicks\"]].groupby(by=[\"linkTarget\"]).sum()\n",
    "articles[\"CTR\"] = articles[\"clicks\"]/articles[\"impressions\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_csv('./output/output_articles.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Wikispeedia network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better understanding of the structure of the links, we wish to create a graph of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G = nx.from_pandas_edgelist(df_links, source='linkSource', target='linkTarget')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.sparse' has no attribute 'coo_array'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw_networkx\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwith_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_color\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m#001f3f\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_color\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m#AAAAAA\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:331\u001B[0m, in \u001B[0;36mdraw_networkx\u001B[1;34m(G, pos, arrows, with_labels, **kwds)\u001B[0m\n\u001B[0;32m    328\u001B[0m label_kwds \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwds\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m valid_label_kwds}\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pos \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 331\u001B[0m     pos \u001B[38;5;241m=\u001B[39m \u001B[43mnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrawing\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspring_layout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# default to spring layout\u001B[39;00m\n\u001B[0;32m    333\u001B[0m draw_networkx_nodes(G, pos, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mnode_kwds)\n\u001B[0;32m    334\u001B[0m draw_networkx_edges(G, pos, arrows\u001B[38;5;241m=\u001B[39marrows, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39medge_kwds)\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\networkx\\utils\\decorators.py:816\u001B[0m, in \u001B[0;36margmap.__call__.<locals>.func\u001B[1;34m(_argmap__wrapper, *args, **kwargs)\u001B[0m\n\u001B[0;32m    815\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(\u001B[38;5;241m*\u001B[39margs, __wrapper\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 816\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m argmap\u001B[38;5;241m.\u001B[39m_lazy_compile(__wrapper)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m<class 'networkx.utils.decorators.argmap'> compilation 8:4\u001B[0m, in \u001B[0;36margmap_spring_layout_5\u001B[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m splitext\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcontextlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m contextmanager\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnetworkx\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnx\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnetworkx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_random_state, create_py_random_state\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\networkx\\drawing\\layout.py:476\u001B[0m, in \u001B[0;36mspring_layout\u001B[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001B[0m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(G) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m500\u001B[39m:  \u001B[38;5;66;03m# sparse solver for large graphs\u001B[39;00m\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m\n\u001B[1;32m--> 476\u001B[0m A \u001B[38;5;241m=\u001B[39m \u001B[43mnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_scipy_sparse_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m fixed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;66;03m# We must adjust k by domain size for layouts not near 1x1\u001B[39;00m\n\u001B[0;32m    479\u001B[0m     nnodes, _ \u001B[38;5;241m=\u001B[39m A\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[1;32mC:\\anaconda3\\lib\\site-packages\\networkx\\convert_matrix.py:921\u001B[0m, in \u001B[0;36mto_scipy_sparse_array\u001B[1;34m(G, nodelist, dtype, weight, format)\u001B[0m\n\u001B[0;32m    919\u001B[0m         r \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m diag_index\n\u001B[0;32m    920\u001B[0m         c \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m diag_index\n\u001B[1;32m--> 921\u001B[0m     A \u001B[38;5;241m=\u001B[39m \u001B[43msp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoo_array\u001B[49m((d, (r, c)), shape\u001B[38;5;241m=\u001B[39m(nlen, nlen), dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m    922\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    923\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m A\u001B[38;5;241m.\u001B[39masformat(\u001B[38;5;28mformat\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'scipy.sparse' has no attribute 'coo_array'"
     ]
    }
   ],
   "source": [
    "nx.draw_networkx(G, with_labels=False, node_size=5, node_color=\"#001f3f\", width=0.1, edge_color=\"#AAAAAA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we distinguish two strongly separate connected components. We can verify that easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \" + str(nx.number_connected_components(G)) + \"  connected components in the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unconnected components: \"+ str(list(nx.connected_components(G))[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is linked to pages for donations, these are not Wikipedia articles and will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset we have obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There exist', len(df_links), 'links in total.','\\n')\n",
    "print('The unique articles are:', df_links.linkSource.nunique())\n",
    "df_links['linkSource'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_links = df_links.groupby(by=\"linkSource\").count().reset_index()[:50]\n",
    "\n",
    "pl = group_by_links.plot(kind='bar', figsize=[11,5], rot=0)\n",
    "pl.set_title('Distribution of the number of links per article')\n",
    "pl.set_ylabel('Number of links')\n",
    "pl.set_xlabel('Article')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of links per article does not follow a uniform distribution. There are articles that have more than 160 links while others have less than 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[\"clicks\"].plot.hist(bins=30, range=[0,1300], title=\"Number of clicks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rank the top 10 most clicks articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.sort_values(by=['clicks'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now focusing on impressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[\"impressions\"].plot.hist(bins=30, title=\"Number of impressions\", range=[0,50000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rank the top 10 articles with highest impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.sort_values(by=['impressions'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finaly analysis is made on CTR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[\"CTR\"].plot.hist(bins=30, title=\"Distribution of CTR\", range=[0,0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the top 10 CTR ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.sort_values(by=['CTR'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last ranking might not be very useful since some articles have very low impressions, meaning we don't have enough data. The obtained CTR seem very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average number of clicks is \" + str(articles.clicks.mean()))\n",
    "print(\"The average number of impressions is \" + str(articles.impressions.mean()))\n",
    "print(\"The average CTR is \" + str(articles.CTR.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check how clean how dataset is, and if we have missing data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
